{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"solr_bert_squad_Dipanjan.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"py37_pytorch","language":"python","name":"conda-env-py37_pytorch-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_x7n2hvQlCbE"},"source":["Set up some parameters in the kernel"]},{"cell_type":"code","metadata":{"id":"xQv7kqlYlCbI"},"source":["USE_SUMMARY = True\n","FIND_PDFS = False\n","SEARCH_MEDRXIV = False\n","SEARCH_PUBMED = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98gzW5stlCbT"},"source":["First we need to go get openJDK 11 set up"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5-iAgnTuFLj","executionInfo":{"status":"ok","timestamp":1615121134115,"user_tz":-330,"elapsed":2727,"user":{"displayName":"Dipanjan Chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZV8ta2PugBqO1BRDCS8Xf-fLx-hHufCftdjqImQ=s64","userId":"04118492169344148611"}},"outputId":"ce89d8c6-1d7c-4637-b996-787d68505f49"},"source":["!java -version"],"execution_count":null,"outputs":[{"output_type":"stream","text":["openjdk version \"11.0.10\" 2021-01-19\n","OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04)\n","OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"6Ul4KFJWlCbW","scrolled":true,"executionInfo":{"status":"ok","timestamp":1615121134116,"user_tz":-330,"elapsed":2720,"user":{"displayName":"Dipanjan Chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZV8ta2PugBqO1BRDCS8Xf-fLx-hHufCftdjqImQ=s64","userId":"04118492169344148611"}},"outputId":"08b84ec1-c25a-4ee5-b861-662584f25ef6"},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2/\"\n","print(os.environ[\"JAVA_HOME\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/lib/jvm/jdk-11.0.2/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yNYicH0dlCbk"},"source":["Now lets get Pyserini (python wrapped Anserini) setup"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/","height":537},"id":"jsnSO1rClCbn","scrolled":true,"executionInfo":{"status":"ok","timestamp":1615121164101,"user_tz":-330,"elapsed":15662,"user":{"displayName":"Dipanjan Chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZV8ta2PugBqO1BRDCS8Xf-fLx-hHufCftdjqImQ=s64","userId":"04118492169344148611"}},"outputId":"5cc5efb6-e23d-48c4-85f8-3ac61d227d25"},"source":["#%%capture\n","# !pip install pyserini==0.8.1.0\n","# !pip install transformers\n","from pyserini.search import pysearch"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SystemError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-aa2b35278db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyserini==0.8.1.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpysearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyserini/search/pysearch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyclass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJSearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJArrayList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJTopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJTopicReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyserini/pyclass.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mconfigure_classpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ANSERINI_CLASSPATH'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'ANSERINI_CLASSPATH'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resources/jars/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjnius\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautoclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jnius/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjnius\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreflect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwith_metaclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jnius/reflect.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_metaclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMetaJavaClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0m__javaclass__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'java/lang/Class'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, this_bases, d)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0mresolved_bases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolved_bases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mjnius/jnius_export_class.pxi\u001b[0m in \u001b[0;36mjnius.MetaJavaClass.__new__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mjnius/jnius_export_class.pxi\u001b[0m in \u001b[0;36mjnius.MetaJavaClass.resolve_class\u001b[0;34m()\u001b[0m\n","\u001b[0;32mjnius/jnius_env.pxi\u001b[0m in \u001b[0;36mjnius.get_jnienv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mjnius/jnius_jvm_dlopen.pxi\u001b[0m in \u001b[0;36mjnius.get_platform_jnienv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mjnius/jnius_jvm_dlopen.pxi\u001b[0m in \u001b[0;36mjnius.create_jnienv\u001b[0;34m()\u001b[0m\n","\u001b[0;31mSystemError\u001b[0m: Error calling dlopen(b'/usr/lib/jvm/jdk-11.0.2/lib/server/libjvm.so': b'/usr/lib/jvm/jdk-11.0.2/lib/server/libjvm.so: cannot open shared object file: No such file or directory'"]}]},{"cell_type":"markdown","metadata":{"id":"VgxcHqD_lCby"},"source":["Now we need the lucene searchable CORD-19 database"]},{"cell_type":"markdown","metadata":{"id":"6eGCnwiElCb-"},"source":["Now Lets get the Universal Sentence Encoder"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"tMDLPOGZlCcA"},"source":["# import tensorflow as tf\n","# import tensorflow_hub as hub\n","# !mkdir -p /content/sentence_wise_email/module/module_useT\n","# # Download the module, and uncompress it to the destination folder. \n","# !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC /content/sentence_wise_email/module/module_useT/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uuuxz6V3lCcH"},"source":["now lets get the transformer models"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"lD192t2NlCcI"},"source":["import torch\n","from transformers import BertForQuestionAnswering\n","from transformers import BertTokenizer\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","torch_device = 'cpu'\n","\n","#---?\n","QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad') -- where are the models\n","QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","QA_MODEL.to(torch_device)\n","QA_MODEL.eval()\n","\n","if USE_SUMMARY:\n","    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","    SUMMARY_MODEL.to(torch_device)\n","    SUMMARY_MODEL.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nRf5pOWlCcV"},"source":["now lets get metapub to be able to find pdfs if available"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"W-3peUf6lCcX"},"source":["if FIND_PDFS:\n","    !pip install metapub"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3BLA3madlCck"},"source":["Now lets get biopython set up so we can go search pubmed if we want to"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"kPMBqRnPlCcm"},"source":["try:\n","    from StringIO import StringIO\n","except ImportError:\n","    from io import StringIO\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wV1vj4SWlCcw"},"source":["# Now we should be set to make our first query of the CORD-19 database.  Lets look at the top 10 results based on our query.\n","\n","lest break our query up into two parts:\n","\n","1) a natural language question\n","\n","2) a set of keywords that can help drive the answerini search enginge towards the most interesting results for the question we want to ask\n","\n","This is beneficial becuase the answerini portion of the search is not really contextual and cant discipher meaning so the keywords will help drive the search.  This could be refined eventually by using a BERT model to create an embedding from the question being asked.  For right now, this is good enough."]},{"cell_type":"code","metadata":{"id":"I43azuBjt_Kb"},"source":["from urllib.request import urlopen\n","import json\n","\n","query = \"what%20is%20the%20company%20revenue\"\n","\n","connection = urlopen('http://localhost:8983/solr/data01/select?q='+query+'&df=Article&wt=json') # write as json and focus on Article Entity #30k \n","response = json.load(connection)\n","print (response['response']['numFound'], \"documents found.\")\n","\n","print(len(response['response']['docs']))\n","\n","# Print the name of each document.\n","for document in response['response']['docs']:\n","#     res = json.loads(document) \n","    print (\"Title =\", document['Title'])\n","    print (\"Company =\", document['Company'])\n","    print(document.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0__ilIzzt_Kb"},"source":["hit_dictionary = {}\n","for document in response['response']['docs']: \n","    data = {}\n","    data['Company_Id'] = document['Company_Id'][0]\n","    data['Company'] = document['Company'][0]\n","    data['Source'] = document['Source'][0]\n","    data['URL'] = document['URL'][0]\n","    data['Title'] = document['Title'][0]\n","    data['Article'] = document['Article'][0]\n","    data['id'] = document['id'][0]\n","    data['Published_Date'] = document['Published_Date'][0]\n","    data['Scrape_Date'] = document['Scrape_Date'][0]\n","    idx = str(document['Unique_id'][0])\n","    hit_dictionary[idx] = data\n","\n","print(hit_dictionary.keys())\n","\n","# scrub the abstracts in prep for BERT-SQuAD\n","for idx, v in hit_dictionary.items():\n","    v['abstract_full'] = v['Article']\n","#     abs_dirty = v['Article']\n","#     # looks like the abstract value can be an empty list\n","#     v['abstract_paragraphs'] = []\n","#     v['abstract_full'] = ''\n","\n","#     if abs_dirty:\n","#         # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n","#         # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n","#         # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n","\n","\n","#         if isinstance(abs_dirty, list):\n","#             for p in abs_dirty:\n","#                 v['abstract_paragraphs'].append(p['text'])\n","#                 v['abstract_full'] += p['text'] + ' \\n\\n'\n","\n","#         # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n","#         if isinstance(abs_dirty, str):\n","#             v['abstract_paragraphs'].append(abs_dirty)\n","#             v['abstract_full'] += abs_dirty + ' \\n\\n'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzd7vCs6t_Kc"},"source":["print(hit_dictionary['181664'], '\\n')\n","print(hit_dictionary['203777'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NnjTQS7RlCdK"},"source":["# Build a semantic similarity search capability to rank answers in terms of how closely they line up to the meaning of the NL question"]},{"cell_type":"markdown","metadata":{"id":"8tAIAFuHlCdM"},"source":["See this [notebook](https://www.kaggle.com/dirktheeng/universal-sentence-encoder-for-nlp-matching) for a stripped own example."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"87M80eNKlCdO"},"source":["# def embed_useT(module):\n","#     with tf.Graph().as_default():\n","#         sentences = tf.compat.v1.placeholder(tf.string)\n","#         embed = hub.Module(module)\n","#         embeddings = embed(sentences)\n","#         session = tf.compat.v1.train.MonitoredSession()\n","#     return lambda x: session.run(embeddings, {sentences: x})\n","# embed_fn = embed_useT('/content/sentence_wise_email/module/module_useT')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"vf1Pq5a5lCdX"},"source":["import numpy as np\n","def reconstructText(tokens, start=0, stop=-1):\n","    tokens = tokens[start: stop]\n","    if '[SEP]' in tokens:\n","        sepind = tokens.index('[SEP]')\n","        tokens = tokens[sepind+1:]\n","    txt = ' '.join(tokens)\n","    txt = txt.replace(' ##', '')\n","    txt = txt.replace('##', '')\n","    txt = txt.strip()\n","    txt = \" \".join(txt.split())\n","    txt = txt.replace(' .', '.')\n","    txt = txt.replace('( ', '(')\n","    txt = txt.replace(' )', ')')\n","    txt = txt.replace(' - ', '-')\n","    txt_list = txt.split(' , ')\n","    txt = ''\n","    nTxtL = len(txt_list)\n","    if nTxtL == 1:\n","        return txt_list[0]\n","    newList =[]\n","    for i,t in enumerate(txt_list):\n","        if i < nTxtL -1:\n","            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n","                newList += [t,',']\n","            else:\n","                newList += [t, ', ']\n","        else:\n","            newList += [t]\n","    return ''.join(newList)\n","\n","\n","def makeBERTSQuADPrediction(document, question):\n","    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n","    ## 50 word overlaps on either end so that it can understand and check longer abstracts\n","    nWords = len(document.split())\n","    input_ids_all = QA_TOKENIZER.encode(question, document)\n","    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n","    overlapFac = 1.1\n","    if len(input_ids_all)*overlapFac > 2048:\n","        nSearchWords = int(np.ceil(nWords/5))\n","        quarter = int(np.ceil(nWords/4))\n","        docSplit = document.split()\n","        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n","                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n","                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n","                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n","        \n","    elif len(input_ids_all)*overlapFac > 1536:\n","        nSearchWords = int(np.ceil(nWords/4))\n","        third = int(np.ceil(nWords/3))\n","        docSplit = document.split()\n","        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                     ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n","                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n","                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n","        \n","    elif len(input_ids_all)*overlapFac > 1024:\n","        nSearchWords = int(np.ceil(nWords/3))\n","        middle = int(np.ceil(nWords/2))\n","        docSplit = document.split()\n","        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n","                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","    elif len(input_ids_all)*overlapFac > 512:\n","        nSearchWords = int(np.ceil(nWords/2))\n","        docSplit = document.split()\n","        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","    else:\n","        input_ids = [input_ids_all]\n","    absTooLong = False    \n","    \n","    answers = []\n","    cons = []\n","    for iptIds in input_ids:\n","        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n","        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n","        num_seg_a = sep_index + 1\n","        num_seg_b = len(iptIds) - num_seg_a\n","        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","        assert len(segment_ids) == len(iptIds)\n","        n_ids = len(segment_ids)\n","        #print(n_ids)\n","\n","        if n_ids < 512:\n","            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n","                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n","        else:\n","            #this cuts off the text if its more than 512 words so it fits in model space\n","            #need run multiple inferences for longer text. add to the todo\n","            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n","            absTooLong = True\n","            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n","                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n","        start_scores = start_scores[:,1:-1]\n","        end_scores = end_scores[:,1:-1]\n","        answer_start = torch.argmax(start_scores)\n","        answer_end = torch.argmax(end_scores)\n","        #print(answer_start, answer_end)\n","        answer = reconstructText(tokens, answer_start, answer_end+2)\n","    \n","        if answer.startswith('. ') or answer.startswith(', '):\n","            answer = answer[2:]\n","            \n","        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n","        answers.append(answer)\n","        cons.append(c)\n","    \n","    maxC = max(cons)\n","    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n","    confidence = cons[iMaxC]\n","    answer = answers[iMaxC]\n","    \n","    sep_index = tokens_all.index('[SEP]')\n","    full_txt_tokens = tokens_all[sep_index+1:]\n","    \n","    abs_returned = reconstructText(full_txt_tokens)\n","\n","    ans={}\n","    ans['answer'] = answer\n","    #print(answer)\n","    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n","        ans['confidence'] = -1000000\n","    else:\n","        #confidence = torch.max(start_scores) + torch.max(end_scores)\n","        #confidence = np.log(confidence.item())\n","        ans['confidence'] = confidence\n","    #ans['start'] = answer_start.item()\n","    #ans['end'] = answer_end.item()\n","    ans['abstract_bert'] = abs_returned\n","    ans['abs_too_long'] = absTooLong\n","    return ans"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUbpn96glCdf"},"source":["Now we can write a function to do an Open Domain QA on all the abstracts"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"iehfHFKJlCdg"},"source":["from tqdm import tqdm\n","def searchAbstracts(hit_dictionary, question):\n","    abstractResults = {}\n","    otherResults = {}\n","    for k,v in tqdm(hit_dictionary.items()):\n","        abstract = v['abstract_full']\n","        if abstract:\n","            ans = makeBERTSQuADPrediction(abstract, question)\n","#             print(ans)\n","            if ans['answer']:\n","#                 print(\"check \", k, ans)\n","                confidence = ans['confidence']\n","                abstractResults[confidence]={}\n","                abstractResults[confidence]['answer'] = ans['answer']\n","                #abstractResults[confidence]['start'] = ans['start']\n","                #abstractResults[confidence]['end'] = ans['end']\n","                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n","                abstractResults[confidence]['idx'] = k\n","                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n","                abstractResults[confidence]['Company'] = v['Company']\n","                abstractResults[confidence]['Source'] = v['Source']\n","                abstractResults[confidence]['Title'] = v['Title']\n","                abstractResults[confidence]['Published_Date'] = v['Published_Date']\n","            else:\n","#                 print(\"check \", k, ans)\n","                confidence = ans['confidence']+int(k)\n","                otherResults[confidence]={}\n","#                 otherResults[confidence]['answer'] = ans['answer']\n","                #otherResults[confidence]['start'] = ans['start']\n","                #otherResults[confidence]['end'] = ans['end']\n","                otherResults[confidence]['abstract_bert'] = ans['abstract_bert']\n","                otherResults[confidence]['idx'] = k\n","                otherResults[confidence]['abs_too_long'] = ans['abs_too_long']\n","                otherResults[confidence]['Company'] = v['Company']\n","                otherResults[confidence]['Source'] = v['Source']\n","                otherResults[confidence]['Title'] = v['Title']\n","                otherResults[confidence]['Published_Date'] = v['Published_Date']\n","    \n","    cList = list(abstractResults.keys())\n","    \n","    if cList:\n","        maxScore = max(cList)\n","        total = 0.0\n","        exp_scores = []\n","        for c in cList:\n","            s = np.exp(c-maxScore)\n","            exp_scores.append(s)\n","        total = sum(exp_scores)\n","        for i,c in enumerate(cList):\n","            abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n","    \n","    cList = list(otherResults.keys())\n","    \n","    if cList:\n","        maxScore = max(cList)\n","        total = 0.0\n","        exp_scores = []\n","        for c in cList:\n","            s = np.exp(c-maxScore)\n","            exp_scores.append(s)\n","        total = sum(exp_scores)\n","        for i,c in enumerate(cList):\n","            otherResults[exp_scores[i]/total] = otherResults.pop(c)\n","    \n","    return abstractResults, otherResults"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Nw15wRIqt_Kn"},"source":["query = 'what is the company revenue'\n","answers, other_ans = searchAbstracts(hit_dictionary, query)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYw9FHHlt_Ko"},"source":["print(len(answers), len(other_ans))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ylpID25t_Ko"},"source":["# answers\n","# with open('result_output.json', 'w') as outfile:\n","#     json.dump(answers, outfile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bt0obDf4lCdy"},"source":["Lets put this together in a more eye pleasing way\n","\n","I noticed that the more confident the BERT-SQuAD is, the less text it seems to highlight.  To make sure that we get the full human understandable concept highlighted, I will set it to highlight the sentance that BERT-SQuAD identified."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"BT-7REnhlCd0"},"source":["# workingPath = ''\n","import pandas as pd\n","if FIND_PDFS:\n","    from metapub import UrlReverse\n","    from metapub import FindIt\n","from IPython.core.display import display, HTML\n","\n","#from summarizer import Summarizer\n","#summarizerModel = Summarizer()\n","def displayResults(hit_dictionary, answers, question):\n","    \n","    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n","    #all_HTML_txt = question_HTML\n","    confidence = list(answers.keys())\n","    confidence.sort(reverse=True)\n","    \n","    confidence = list(answers.keys())\n","    confidence.sort(reverse=True)\n","    \n","\n","    for c in confidence:\n","        if c > 0 and c <= 1 and len(answers[c]['answer']) != 0:\n","            if 'idx' not in  answers[c]:\n","                continue\n","            rowData = []\n","            idx = answers[c]['idx']\n","            company = hit_dictionary[idx]['Company']\n","            source = hit_dictionary[idx]['Source']\n","            title = hit_dictionary[idx]['Title']\n","            date = hit_dictionary[idx]['Published_Date']\n","            \n","            full_abs = answers[c]['abstract_bert']\n","            bert_ans = answers[c]['answer']\n","            \n","            split_abs = full_abs.split(bert_ans)\n","            sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n","            if len(split_abs) == 1:\n","                sentance_end_pos = len(full_abs)\n","                sentance_end =''\n","            else:\n","                sentance_end_pos = split_abs[1].find('. ')+1\n","                if sentance_end_pos == 0:\n","                    sentance_end = split_abs[1]\n","                else:\n","                    sentance_end = split_abs[1][:sentance_end_pos]\n","                \n","            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n","            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n","            answers[c]['sentence_beginning'] = sentance_beginning\n","            answers[c]['sentence_end'] = sentance_end\n","            answers[c]['company'] = company\n","            answers[c]['source'] = source\n","            answers[c]['title'] = title\n","            answers[c]['doi'] = date\n","            if 'pdfLink' in hit_dictionary[idx]:\n","                answers[c]['pdfLink'] = hit_dictionary[idx]['pdfLink']\n","                \n","        else:\n","            answers.pop(c)\n","    \n","    \n","    ## now rerank based on semantic similarity of the answers to the question\n","#     cList = list(answers.keys())\n","#     allAnswers = [answers[c]['full_answer'] for c in cList]\n","    \n","#     messages = [question]+allAnswers\n","    \n","#     encoding_matrix = embed_fn(messages)\n","#     similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n","#     rankings = similarity_matrix[1:,0]\n","    \n","#     for i,c in enumerate(cList):\n","#         answers[rankings[i]] = answers.pop(c)\n","\n","    ## now form pandas dv\n","    confidence = list(answers.keys())\n","    confidence.sort(reverse=True)\n","    pandasData = []\n","    ranked_aswers = []\n","    for c in confidence:\n","        rowData=[]\n","        company = answers[c]['company']\n","        source = answers[c]['source']\n","        title = answers[c]['title']\n","        doi = answers[c]['doi']\n","        idx = answers[c]['idx']\n","        rowData += [idx]            \n","        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n","        \n","        rowData += [sentance_html, c, doi]\n","        pandasData.append(rowData)\n","        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n","    \n","    if FIND_PDFS or SEARCH_MEDRXIV:\n","        pdata2 = []\n","        for rowData in pandasData:\n","            rd = rowData\n","            idx = rowData[0]\n","            if 'pdfLink' in answers[rowData[2]]:\n","                rd += ['<a href=\"'+answers[rowData[2]]['pdfLink']+'\" target=\"_blank\">PDF Link</a>']\n","            elif FIND_PDFS:\n","                if str(idx).startswith('pm_'):\n","                    pmid = idx[3:]\n","                else:\n","                    try:\n","                        test = UrlReverse('https://doi.org/'+hit_dictionary[idx]['doi'])\n","                        if test is not None:\n","                            pmid = test.pmid\n","                        else:\n","                            pmid = None\n","                    except:\n","                        pmid = None\n","                pdfLink = None\n","                if pmid is not None:\n","                    try:\n","                        pdfLink = FindIt(str(pmid))\n","                    except:\n","                        pdfLink = None\n","                if pdfLink is not None:\n","                    pdfLink = pdfLink.url\n","\n","                if pdfLink is None:\n","\n","                    rd += ['Not Available']\n","                else:\n","                    rd += ['<a href=\"'+pdfLink+'\" target=\"_blank\">PDF Link</a>']\n","            else:\n","                rd += ['Not Available']\n","            pdata2.append(rowData)\n","    else:\n","        pdata2 = pandasData\n","        \n","    \n","    display(HTML(question_HTML))\n","    \n","    if USE_SUMMARY:\n","        ## try generating an exacutive summary with extractive summarizer\n","        allAnswersTxt = ' '.join(ranked_aswers[:6]).replace('\\n','')\n","    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n","     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n","\n","        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n","        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n","                                               num_beams=10,\n","                                               length_penalty=1.2,\n","                                               max_length=1024,\n","                                               min_length=64,\n","                                               no_repeat_ngram_size=4)\n","\n","        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n","        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n","        display(HTML(execSum_HTML))\n","        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n","        display(HTML(warning_HTML))\n","\n","#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n","    \n","    if FIND_PDFS or SEARCH_MEDRXIV:\n","        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Published Date','PDF Link'])\n","    else:\n","        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Published Date'])\n","        \n","    display(HTML(df.to_html(render_links=True, escape=False)))\n","    \n","#displayResults(hit_dictionary, answers, query)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E0z1ad57t_Kp"},"source":["question = 'what is the company revenue'\n","displayResults(hit_dictionary, answers, question)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TiRaEX8t_Kp"},"source":["import pandas as pd\n","if FIND_PDFS:\n","    from metapub import UrlReverse\n","    from metapub import FindIt\n","from IPython.core.display import display, HTML\n","\n","def displayOtherResults(hit_dictionary, answers, question):\n","    \n","    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n","    #all_HTML_txt = question_HTML\n","    confidence = list(answers.keys())\n","    confidence.sort(reverse=True)\n","    print(len(confidence))\n","    \n","    confidence = list(answers.keys())\n","    confidence.sort(reverse=True)\n","    \n","\n","    for c in confidence:\n","        if len(answers[c]['abstract_bert']) != 0:\n","            if 'idx' not in  answers[c]:\n","                continue\n","            rowData = []\n","            idx = answers[c]['idx']\n","            company = hit_dictionary[idx]['Company']\n","            source = hit_dictionary[idx]['Source']\n","            title = hit_dictionary[idx]['Title']\n","            date = hit_dictionary[idx]['Published_Date']\n","            \n","#             full_abs = answers[c]['abstract_bert']\n","#             bert_ans = answers[c]['answer']\n","            \n","#             split_abs = full_abs.split(bert_ans)\n","#             sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n","#             if len(split_abs) == 1:\n","#                 sentance_end_pos = len(full_abs)\n","#                 sentance_end =''\n","#             else:\n","#                 sentance_end_pos = split_abs[1].find('. ')+1\n","#                 if sentance_end_pos == 0:\n","#                     sentance_end = split_abs[1]\n","#                 else:\n","#                     sentance_end = split_abs[1][:sentance_end_pos]\n","                \n","            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n","            answers[c]['full_answer'] = answers[c]['abstract_bert']\n","            answers[c]['company'] = company\n","            answers[c]['source'] = source\n","            answers[c]['title'] = title\n","            answers[c]['doi'] = date\n","            if 'pdfLink' in hit_dictionary[idx]:\n","                answers[c]['pdfLink'] = hit_dictionary[idx]['pdfLink']\n","                \n","        else:\n","            answers.pop(c)\n","    \n","    \n","    ## now rerank based on semantic similarity of the answers to the question\n","#     cList = list(answers.keys())\n","#     allAnswers = [answers[c]['full_answer'] for c in cList]\n","    \n","#     messages = [question]+allAnswers\n","    \n","#     encoding_matrix = embed_fn(messages)\n","#     similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n","#     rankings = similarity_matrix[1:,0]\n","    \n","#     for i,c in enumerate(cList):\n","#         answers[rankings[i]] = answers.pop(c)\n","\n","    ## now form pandas dv\n","    confidence = list(answers.keys())\n","    confidence.sort(reverse=True)\n","    pandasData = []\n","    ranked_aswers = []\n","    for c in confidence:\n","        rowData=[]\n","        company = answers[c]['company']\n","        source = answers[c]['source']\n","        title = answers[c]['title']\n","        doi = answers[c]['doi']\n","        idx = answers[c]['idx']\n","        rowData += [idx]            \n","        sentance_html = '<div>' +answers[c]['full_answer']+' </div>'\n","        \n","        rowData += [sentance_html, c, doi]\n","        pandasData.append(rowData)\n","        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n","    \n","    \n","    pdata2 = pandasData    \n","    \n","    display(HTML(question_HTML))\n","    \n","    if USE_SUMMARY:\n","        ## try generating an exacutive summary with extractive summarizer\n","        allAnswersTxt = ' '.join(ranked_aswers).replace('\\n','')\n","    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n","     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n","\n","        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n","        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n","                                               num_beams=10,\n","                                               length_penalty=1.2,\n","                                               max_length=1024,\n","                                               min_length=64,\n","                                               no_repeat_ngram_size=4)\n","\n","        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n","        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n","        display(HTML(execSum_HTML))\n","        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n","        display(HTML(warning_HTML))\n","\n","#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n","    \n","    if FIND_PDFS or SEARCH_MEDRXIV:\n","        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Published Date','PDF Link'])\n","    else:\n","        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Published Date'])\n","        \n","    display(HTML(df.to_html(render_links=True, escape=False)))\n","    \n","#displayResults(hit_dictionary, answers, query)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTV-PcrUt_Kp"},"source":["question = 'what is the company revenue'\n","displayOtherResults(hit_dictionary, other_ans, question)\n","# other_ans"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GKvM-W02lCd_"},"source":["Lets search pubmed too to fill in the gaps and get the latest papers that may not be in the lucene database"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"8eOftGIVlCeO"},"source":["def searchDatabase(question):\n","    \n","    query = question.replace(\" \", \"%20\")\n","    print(query)\n","    # query = \"what%20is%20the%20company%20revenue\"\n","    connection = urlopen('http://localhost:8983/solr/robust04/select?q='+query+'&df=Article&wt=json')\n","    response = json.load(connection)\n","    print(len(response['response']['docs']), \"documents found.\")\n","    ## collect the relevant data in a hit dictionary\n","    hit_dictionary = {}\n","    for document in response['response']['docs']:\n","        data = {}\n","        data['Company_Id'] = document['Company_Id'][0]\n","        data['Company'] = document['Company'][0]\n","        data['Source'] = document['Source'][0]\n","        data['URL'] = document['URL'][0]\n","        data['Title'] = document['Title'][0]\n","        data['Article'] = document['Article'][0]\n","        data['id'] = document['id'][0]\n","        data['Published_Date'] = document['Published_Date'][0]\n","        data['Scrape_Date'] = document['Scrape_Date'][0]\n","        idx = str(document['Unique_id'][0])\n","        hit_dictionary[idx] = data\n","\n","    print(hit_dictionary.keys())\n","\n","    ## scrub the abstracts in prep for BERT-SQuAD\n","    for idx,v in hit_dictionary.items():\n","        v['abstract_full'] = v['Article']\n","#         abs_dirty = v['Article']\n","#         # looks like the abstract value can be an empty list\n","#         v['abstract_paragraphs'] = []\n","#         v['abstract_full'] = ''\n","\n","#         if abs_dirty:\n","#             # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n","#             # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n","#             # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n","#             if isinstance(abs_dirty, list):\n","#                 for p in abs_dirty:\n","#                     v['abstract_paragraphs'].append(p['text'])\n","#                     v['abstract_full'] += p['text'] + ' \\n\\n'\n","\n","#             # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n","#             if isinstance(abs_dirty, str):\n","#                 v['abstract_paragraphs'].append(abs_dirty)\n","#                 v['abstract_full'] += abs_dirty + ' \\n\\n'\n","#     print(question, hit_dictionary.keys())\n","    ## Search collected abstracts with BERT-SQuAD\n","    answers, other_ans = searchAbstracts(hit_dictionary, question)\n","#     print(answers)\n","    ## display results in a nice format\n","    displayResults(hit_dictionary, answers, question)\n","    displayOtherResults(hit_dictionary, other_ans, question)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULFl7YWvlCeV"},"source":["Lets try this with the same question and kw to see if it produces the same results we just got"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"a58U2xFblCeX"},"source":["#searchDatabase(query, keywords, pysearch, luceneDir, minDate=minDate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrsN7tXhlCed"},"source":["Great that worked as expected.  Now lets try some new questions"]},{"cell_type":"markdown","metadata":{"id":"d1cTWWjdlCeo"},"source":["# Use this block to refine specific questions before adding them to the list of all questions"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"LNNI3hZGlCeq","scrolled":false},"source":["question = \"what is the company revenue\"\n","\n","searchDatabase(question)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"x81PsLsRt_Kr"},"source":["question = \"Is the company suspected or accused of wrongdoing\"\n","\n","searchDatabase(question)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"OkSb6Jzxt_Kr"},"source":["question = \"Has there been a privacy breach\"\n","\n","searchDatabase(question)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0MEi03Ft_Ks"},"source":["# !pip install tensorflow-text\n","import tensorflow_text as text  # Import TF.text ops\n","import tensorflow.compat.v2 as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","\n","loaded = hub.load(\"https://tfhub.dev/google/LAReQA/mBERT_En_En/1\")\n","\n","question_encoder = loaded.signatures[\"query_encoder\"]\n","response_encoder = loaded.signatures[\"response_encoder\"]\n","\n","questions = [\n","    \"When was Chopin born?\",\n","    \"What is the earth's circumference?\",\n","    # Same questions in Chinese:\n","    \"肖邦是什么时候出生的？\",\n","    \"地球的周长是多少？\"]\n","\n","responses = [\n","    \"He was born in 1810.\",\n","    \"Its circumference is 40,075 kilometers.\",\n","    # Same sentences in Chinese:\n","    \"他出生于1810年。\",\n","    \"其周长为40075公里。\"]\n","\n","response_contexts = [\n","    \"Chopin was a Polish-French composer. He was born in 1810.\",\n","    \"The Earth's shape is nearly spherical. Its circumference is 40,075 \"\n","    \"kilometers.\",\n","    # Same contexts in Chinese:\n","    \"肖邦是波兰法国作曲家。 他出生于1810年。\",\n","    \"地球的形状几乎是球形的。 其周长为40075公里。\"]\n","\n","question_embeddings = question_encoder(\n","    input=tf.constant(np.asarray(questions)))[\"outputs\"]\n","\n","response_embeddings = response_encoder(\n","    input=tf.constant(np.asarray(responses)),\n","    context=tf.constant(np.asarray(response_contexts)))[\"outputs\"]\n","\n","scores = np.matmul(question_embeddings.numpy(),\n","                   np.transpose(response_embeddings.numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-YgJrXaCFIP","executionInfo":{"status":"ok","timestamp":1619471735142,"user_tz":-330,"elapsed":863,"user":{"displayName":"Dipanjan Chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZV8ta2PugBqO1BRDCS8Xf-fLx-hHufCftdjqImQ=s64","userId":"04118492169344148611"}},"outputId":"1add4faf-3109-417a-cfba-e3c58b7309ef"},"source":["scores"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.88566893, 0.40130743, 0.74755454, 0.39251137],\n","       [0.34988382, 0.77337795, 0.31081748, 0.61363643],\n","       [0.5688845 , 0.33898512, 0.75303507, 0.3749957 ],\n","       [0.32286048, 0.5961894 , 0.3767861 , 0.79739237]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Souw4ngpCZXE"},"source":["from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","from flask_restful import Resource, Api\n","\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","import re\n","import gc\n","import requests\n","from bs4 import BeautifulSoup\n","import datetime\n","import dateutil.parser as dparser\n","\n","import torch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import torch\n","import transformers\n","from transformers import *\n","from transformers import BertForQuestionAnswering\n","from transformers import BertTokenizer\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","import pyserini\n","from pyserini.search import pysearch\n","from IPython.core.display import display, HTML\n","from tqdm import tqdm\n","from Bio import Entrez, Medline\n","try:\n","    from StringIO import StringIO\n","except ImportError:\n","    from io import StringIO\n","import warnings\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2/\"\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","app = Flask(__name__)\n","cors = CORS(app, resources={r\"*\": {\"origins\": \"*\"}})\n","api = Api(app)\n","\n","# Initailize tensorflow module globally if you have GPU else comment out this part. Please check the no_gpu branch. \n","def embed_useT():\n","   module = '/sentence_wise_email/module/module_useT'\n","   with tf.Graph().as_default():\n","       sentences = tf.compat.v1.placeholder(tf.string)\n","       embed = hub.Module(module)\n","       embeddings = embed(sentences)\n","       session = tf.compat.v1.train.MonitoredSession()\n","   return lambda x: session.run(embeddings, {sentences: x})\n","embed_fn = embed_useT()\n","\n","\n","class BertSquad:\n","\n","    USE_SUMMARY = True\n","    FIND_PDFS = False\n","    SEARCH_MEDRXIV = False\n","    SEARCH_PUBMED = False\n","\n","    minDate = '2020/08/13'\n","    luceneDir = '/data/indexes/lucene-index-cord19/'\n","\n","    torch_device = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","    QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","    QA_MODEL.to(torch_device)\n","    QA_MODEL.eval()\n","\n","    if USE_SUMMARY:\n","        SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","        SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","        SUMMARY_MODEL.to(torch_device)\n","        SUMMARY_MODEL.eval()\n","\n","    para_model = AutoModel.from_pretrained('monologg/biobert_v1.1_pubmed')\n","    para_tokenizer = AutoTokenizer.from_pretrained('monologg/biobert_v1.1_pubmed', do_lower_case=False)\n","    gc.collect()\n","\n","\n","    def reconstructText(self, tokens, start=0, stop=-1):\n","        tokens = tokens[start: stop]\n","        if '[SEP]' in tokens:\n","            sepind = tokens.index('[SEP]')\n","            tokens = tokens[sepind+1:]\n","        txt = ' '.join(tokens)\n","        txt = txt.replace(' ##', '')\n","        txt = txt.replace('##', '')\n","        txt = txt.strip()\n","        txt = \" \".join(txt.split())\n","        txt = txt.replace(' .', '.')\n","        txt = txt.replace('( ', '(')\n","        txt = txt.replace(' )', ')')\n","        txt = txt.replace(' - ', '-')\n","        txt_list = txt.split(' , ')\n","        txt = ''\n","        nTxtL = len(txt_list)\n","        if nTxtL == 1:\n","            return txt_list[0]\n","        newList =[]\n","        for i,t in enumerate(txt_list):\n","            if i < nTxtL -1:\n","                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n","                    newList += [t,',']\n","                else:\n","                    newList += [t, ', ']\n","            else:\n","                newList += [t]\n","        return ''.join(newList)\n","\n","\n","    def makeBERTSQuADPrediction(self, document, question):\n","        nWords = len(document.split())\n","        input_ids_all = self.QA_TOKENIZER.encode(question, document)\n","        tokens_all = self.QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n","        overlapFac = 1.1\n","        if len(input_ids_all)*overlapFac > 2048:\n","            nSearchWords = int(np.ceil(nWords/5))\n","            quarter = int(np.ceil(nWords/4))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                        ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n","                        ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n","                        ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n","                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","            \n","        elif len(input_ids_all)*overlapFac > 1536:\n","            nSearchWords = int(np.ceil(nWords/4))\n","            third = int(np.ceil(nWords/3))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                        ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n","                        ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n","                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","            \n","        elif len(input_ids_all)*overlapFac > 1024:\n","            nSearchWords = int(np.ceil(nWords/3))\n","            middle = int(np.ceil(nWords/2))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                        ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n","                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","        elif len(input_ids_all)*overlapFac > 512:\n","            nSearchWords = int(np.ceil(nWords/2))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","        else:\n","            input_ids = [input_ids_all]\n","        absTooLong = False    \n","        \n","        answers = []\n","        cons = []\n","        for iptIds in input_ids:\n","            tokens = self.QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n","            sep_index = iptIds.index(self.QA_TOKENIZER.sep_token_id)\n","            num_seg_a = sep_index + 1\n","            num_seg_b = len(iptIds) - num_seg_a\n","            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","            assert len(segment_ids) == len(iptIds)\n","            n_ids = len(segment_ids)\n","\n","            if n_ids < 512:\n","                start_scores, end_scores = self.QA_MODEL(torch.tensor([iptIds]).to(self.torch_device),\n","                                        token_type_ids=torch.tensor([segment_ids]).to(self.torch_device))\n","            else:\n","                #this cuts off the text if its more than 512 words so it fits in model space \n","                print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n","                absTooLong = True\n","                start_scores, end_scores = self.QA_MODEL(torch.tensor([iptIds[:512]]).to(self.torch_device),\n","                                        token_type_ids=torch.tensor([segment_ids[:512]]).to(self.torch_device))\n","            start_scores = start_scores[:,1:-1]\n","            end_scores = end_scores[:,1:-1]\n","            answer_start = torch.argmax(start_scores)\n","            answer_end = torch.argmax(end_scores)\n","            answer = self.reconstructText(tokens, answer_start, answer_end+2)\n","        \n","            if answer.startswith('. ') or answer.startswith(', '):\n","                answer = answer[2:]\n","                \n","            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n","            answers.append(answer)\n","            cons.append(c)\n","        \n","        maxC = max(cons)\n","        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n","        confidence = cons[iMaxC]\n","        answer = answers[iMaxC]\n","        \n","        sep_index = tokens_all.index('[SEP]')\n","        full_txt_tokens = tokens_all[sep_index+1:]\n","        \n","        abs_returned = self.reconstructText(full_txt_tokens)\n","\n","        ans={}\n","        ans['answer'] = answer\n","        if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n","            ans['confidence'] = -1000000\n","        else:\n","            ans['confidence'] = confidence\n","        ans['abstract_bert'] = abs_returned\n","        ans['abs_too_long'] = absTooLong\n","        return ans\n","\n","\n","    def searchAbstracts(self, hit_dictionary, question):\n","        abstractResults = {}\n","        for k,v in tqdm(hit_dictionary.items()):\n","            abstract = v['abstract_full']\n","            if abstract:\n","                ans = self.makeBERTSQuADPrediction(abstract, question)\n","                if ans['answer']:\n","                    confidence = ans['confidence']\n","                    abstractResults[confidence]={}\n","                    abstractResults[confidence]['main_abstract'] = abstract\n","                    abstractResults[confidence]['answer'] = ans['answer']\n","                    abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n","                    abstractResults[confidence]['idx'] = k\n","                    abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n","                    \n","        cList = list(abstractResults.keys())\n","        if cList:\n","            maxScore = max(cList)\n","            total = 0.0\n","            exp_scores = []\n","            for c in cList:\n","                s = np.exp(c-maxScore)\n","                exp_scores.append(s)\n","            total = sum(exp_scores)\n","            for i,c in enumerate(cList):\n","                abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n","        return abstractResults\n","\n","\n","    swer']) != 0:\n","                if 'idx' not in  answers[c]:def displayResults(self, hit_dictionary, answers, question):\n","        \n","        question_HTML = '<div font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n","        confidence = list(answers.keys())\n","        confidence.sort(reverse=True)\n","        confidence = list(answers.keys())\n","        confidence.sort(reverse=True)\n","\n","        for c in confidence:\n","            if c>0 and c <= 1 and len(answers[c]['an\n","                    continue\n","                rowData = []\n","                idx = answers[c]['idx']\n","                title = hit_dictionary[idx]['title']\n","                authors = hit_dictionary[idx]['authors'] + ' et al.'\n","                doi = '<a href=\"https://doi.org/'+hit_dictionary[idx]['doi']+'\" target=\"_blank\">' + title +'</a>'\n","                main_abstract = answers[c]['main_abstract']\n","                \n","                full_abs = answers[c]['abstract_bert']\n","                bert_ans = answers[c]['answer']\n","                \n","                split_abs = full_abs.split(bert_ans)\n","                sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n","                if len(split_abs) == 1:\n","                    sentance_end_pos = len(full_abs)\n","                    sentance_end =''\n","                else:\n","                    sentance_end_pos = split_abs[1].find('. ')+1\n","                    if sentance_end_pos == 0:\n","                        sentance_end = split_abs[1]\n","                    else:\n","                        sentance_end = split_abs[1][:sentance_end_pos]\n","                    \n","                answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n","                answers[c]['sentence_beginning'] = sentance_beginning\n","                answers[c]['sentence_end'] = sentance_end\n","                answers[c]['title'] = title\n","                answers[c]['doi'] = doi\n","                answers[c]['main_abstract'] = main_abstract\n","                if 'pdfLink' in hit_dictionary[idx]:\n","                    answers[c]['pdfLink'] = hit_dictionary[idx]['pdfLink']\n","\n","            else:\n","                answers.pop(c)\n","        \n","        # Please check the no_gpu branch. Comment out this part if the system doesn't support GPU\n","        ## re-rank based on semantic similarity of the answers to the question\n","        cList = list(answers.keys())\n","        allAnswers = [answers[c]['full_answer'] for c in cList]\n","\n","        messages = [question]+allAnswers\n","\n","        encoding_matrix = embed_fn(messages)\n","        gc.collect()\n","        similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n","        rankings = similarity_matrix[1:, 0]\n","\n","        for i, c in enumerate(cList):\n","            answers[rankings[i]] = answers.pop(c)\n","            \n","        # Comment till here if required\n","\n","        ## now form pandas dv\n","        confidence = list(answers.keys())\n","        confidence.sort(reverse=True)\n","        pandasData = []\n","        ranked_aswers = []\n","        for c in confidence:\n","            rowData=[]\n","            title = answers[c]['title']\n","            main_abstract = answers[c]['main_abstract']\n","            doi = answers[c]['doi']\n","            idx = answers[c]['idx']\n","            rowData += [idx]            \n","            sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='#08A293'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n","            \n","            rowData += [sentance_html, c, doi, main_abstract]\n","            pandasData.append(rowData)\n","            ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n","        \n","        if self.FIND_PDFS or self.SEARCH_MEDRXIV:\n","            pdata2 = []\n","            for rowData in pandasData:\n","                rd = rowData\n","                idx = rowData[0]\n","                if 'pdfLink' in answers[rowData[2]]:\n","                    rd += ['<a href=\"'+answers[rowData[2]]['pdfLink']+'\" target=\"_blank\">PDF Link</a>']\n","                elif self.FIND_PDFS:\n","                    if str(idx).startswith('pm_'):\n","                        pmid = idx[3:]\n","                    else:\n","                        try:\n","                            test = self.UrlReverse('https://doi.org/'+hit_dictionary[idx]['doi'])\n","                            if test is not None:\n","                                pmid = test.pmid\n","                            else:\n","                                pmid = None\n","                        except:\n","                            pmid = None\n","                    pdfLink = None\n","                    if pmid is not None:\n","                        try:\n","                            pdfLink = self.FindIt(str(pmid))\n","                        except:\n","                            pdfLink = None\n","                    if pdfLink is not None:\n","                        pdfLink = pdfLink.url\n","\n","                    if pdfLink is None:\n","\n","                        rd += ['Not Available']\n","                    else:\n","                        rd += ['<a href=\"'+pdfLink+'\" target=\"_blank\">PDF Link</a>']\n","                else:\n","                    rd += ['Not Available']\n","                pdata2.append(rowData)\n","        else:\n","            pdata2 = pandasData\n","\n","        df = pd.DataFrame(pdata2, columns=['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link', 'Abstract'])\n","        \n","        if self.USE_SUMMARY:\n","            allAnswersTxt = ' '.join(ranked_aswers[:6]).replace('\\n','')\n","            answers_input_ids = self.SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(self.torch_device)\n","            summary_ids = self.SUMMARY_MODEL.generate(answers_input_ids, num_beams=10, length_penalty=1.2, max_length=1024, min_length=64, no_repeat_ngram_size=4)\n","\n","            exec_sum = self.SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n","            execSum_HTML = '<div style=\"font-size:12px;color:#CCCC00\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n","            warning_HTML = '<div style=\"font-size:12px;padding-bottom:12px;color:#CCCC00;margin-top:1px\"> Warning: This is an autogenerated summary based on semantic search of abstracts, please examine the results before accepting this conclusion. There may be scenarios in which the summary will not be able to clearly answer the question.</div>'\n","        \n","        if self.FIND_PDFS or self.SEARCH_MEDRXIV:\n","            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link', 'Abstract'])\n","        else:\n","            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link', 'Abstract'])\n","            \n","        return exec_sum, warning_HTML, df.to_json(orient=\"records\", force_ascii=True, default_handler=None)\n","\n","\n","    def getrecord(self, id, db):\n","        handle = Entrez.efetch(db=db, id=id, rettype='Medline', retmode='text')\n","        rec = handle.read()\n","        handle.close()\n","        return rec\n","\n","\n","    def pubMedSearch(self, terms, db='pubmed', mindate='2019/12/01'):\n","        handle = Entrez.esearch(db = db, term = terms, retmax=10, mindate=mindate)\n","        record = Entrez.read(handle)\n","        record_db = {}\n","        for id in record['IdList']:\n","            try:\n","                record = self.getrecord(id,db)\n","                recfile = StringIO(record)\n","                rec = Medline.read(recfile)\n","                if 'AB' in rec and 'AU' in rec and 'LID' in rec and 'TI' in rec:\n","                    if '10.' in rec['LID'] and ' [doi]' in rec['LID']:\n","                        record_db['pm_'+id] = {}\n","                        record_db['pm_'+id]['authors'] = ' '.join(rec['AU'])\n","                        record_db['pm_'+id]['doi'] = '10.'+rec['LID'].split('10.')[1].split(' [doi]')[0]\n","                        record_db['pm_'+id]['abstract'] = rec['AB']\n","                        record_db['pm_'+id]['title'] = rec['TI']\n","            except:\n","                print(\"Problem trying to retrieve: \" + str(id))\n","            \n","        return record_db\n","    Entrez.email = 'pubmedkaggle@gmail.com'\n","\n","   \n","    def medrxivSearch(self, query, n_pages=1):\n","        results = {}\n","        q = query\n","        for x in range(n_pages):\n","            PARAMS = {\n","                'page': x\n","            }\n","            r = requests.get('https://www.medrxiv.org/search/' + q, params = PARAMS)\n","            content = r.text\n","            page = BeautifulSoup(content, 'lxml')\n","            \n","            for entry in page.find_all(\"a\", attrs={\"class\": \"highwire-cite-linked-title\"}):\n","                title = \"\"\n","                url = \"\"\n","                pubDate = \"\"\n","                journal = None\n","                abstract = \"\"\n","                authors = []\n","                database = \"medRxiv\"\n","                \n","                url = \"https://www.medrxiv.org\" + entry.get('href')\n","                \n","                request_entry = requests.get(url)\n","                content_entry = request_entry.text\n","                page_entry = BeautifulSoup(content_entry, 'lxml')\n","                doi = page_entry.find(\"span\", attrs={\"class\": \"highwire-cite-metadata-doi\"}).text.split('doi.org/')[-1]\n","\n","                #getting pubDate\n","                pubDate = page_entry.find_all(\"div\", attrs = {\"class\": \"pane-content\"})\n","                pubDate = pubDate[10]\n","                pubDate = str(dparser.parse(pubDate, fuzzy = True))\n","                pubDate = datetime.datetime.strptime(pubDate, '%Y-%m-%d %H:%M:%S')\n","                pubDate = pubDate.strftime('%b %d %Y')\n","                date = pubDate.split()\n","                month = date[0]\n","                day = date[1]\n","                year = date[2]\n","                pubDate = {\n","                    'year': year,\n","                    'month': month,\n","                    'day': day\n","                }\n","\n","                #getting title\n","                title = page_entry.find(\"h1\", attrs={\"class\": \"highwire-cite-title\"}).text\n","                #getting abstract\n","                abstract = page_entry.find(\"p\", attrs = {\"id\": \"p-2\"}).text.replace('\\n', ' ')\n","                #getting authors \n","                givenNames = page_entry.find_all(\"span\", attrs={\"class\": \"nlm-given-names\"})\n","                surnames = page_entry.find_all(\"span\",  attrs={\"class\": \"nlm-surname\"})\n","                names = list(zip(givenNames,surnames))\n","                for author in names:\n","                    name = author[0].text + ' ' + author[1].text\n","                    if name not in authors:\n","                        authors.append(name)\n","                \n","                result = {\n","                    'title': title,\n","                    'url': url,\n","                    'pubDate': pubDate,\n","                    'journal': journal,\n","                    'abstract': abstract,\n","                    'authors': authors[0],\n","                    'database': database,\n","                    'doi': doi,\n","                    'pdfLink': url+'.full.pdf'\n","                }\n","                results['mrx_'+result['doi'].split('/')[-1]] = result\n","                #break\n","\n","        return results\n","\n","\n","    def searchDatabase(self, question, keywords, pysearch):\n","        ## search the lucene database with a combination of the question and the keywords\n","        pm_kw = ''\n","        minDate='2019/12/01'\n","        k=20\n","        \n","        searcher = pysearch.SimpleSearcher(self.luceneDir)\n","        hits = searcher.search(question + '. ' + keywords, k=k)\n","        n_hits = len(hits)\n","        ## collect the relevant data in a hit dictionary\n","        hit_dictionary = {}\n","        for i in range(0, n_hits):\n","            doc_json = json.loads(hits[i].raw)\n","            idx = str(hits[i].docid)\n","            hit_dictionary[idx] = doc_json\n","            hit_dictionary[idx]['title'] = hits[i].lucene_document.get(\"title\")\n","            hit_dictionary[idx]['authors'] = hits[i].lucene_document.get(\"authors\")\n","            hit_dictionary[idx]['doi'] = hits[i].lucene_document.get(\"doi\")\n","            \n","        titleList = [h['title'] for h in hit_dictionary.values()]\n","        \n","        # search for PubMed and medArxiv data dynamically\n","        if pm_kw:\n","            if SEARCH_PUBMED:\n","                new_hits = pubMedSearch(pm_kw, db='pubmed', mindate=minDate)\n","                for id,h in new_hits.items():\n","                    if h['title'] not in titleList:\n","                        titleList.append(h['title'])\n","                    hit_dictionary[id] = h\n","            if SEARCH_MEDRXIV:\n","                new_hits = medrxivSearch(pm_kw)\n","                for id,h in new_hits.items():\n","                    if h['title'] not in titleList:\n","                        titleList.append(h['title'])\n","                    hit_dictionary[id] = h\n","        \n","        ## scrub the abstracts in prep for BERT-SQuAD\n","        for idx,v in hit_dictionary.items():\n","\n","            try:\n","                abs_dirty = v['abstract']\n","            except KeyError:\n","                print(\"Sorry! No abstract found.\")\n","                abs_dirty = ''\n","                # uncomment the code if required search on body_text also. Will impact processing time\n","\n","    #             if v['has_full_text'] == True:\n","    #                 print(v['paper_id'])\n","    #                 abs_dirty = v['body_text']\n","    #             else:\n","    #                 print(v.keys())\n","    #         abs_dirty = ''\n","    #         abs_dirty = v['abstract']\n","\n","            # looks like the abstract value can be an empty list\n","            v['abstract_paragraphs'] = []\n","            v['abstract_full'] = ''\n","\n","            if abs_dirty:\n","                # if it is a list, then the only entry is a dictionary where text is in 'text' key it is broken up by paragraph if it is in that form.  \n","                # make lists for every paragraph that is full abstract text as both could be valuable for BERT derrived QA\n","\n","                if isinstance(abs_dirty, list):\n","                    for p in abs_dirty:\n","                        v['abstract_paragraphs'].append(p['text'])\n","                        v['abstract_full'] += p['text'] + ' \\n\\n'\n","\n","                # in some cases the abstract can be straight up text so we can actually leave that alone\n","                if isinstance(abs_dirty, str):\n","                    v['abstract_paragraphs'].append(abs_dirty)\n","                    v['abstract_full'] += abs_dirty + ' \\n\\n'\n","        \n","        ## Search collected abstracts with BERT-SQuAD\n","        answers = self.searchAbstracts(hit_dictionary, question)\n","        ## displaying results in a nice format\n","        return self.displayResults(hit_dictionary, answers, question)\n","\n","\n","    def show_query(self, query):\n","        \"\"\"HTML print format for the searched query\"\"\"\n","        return HTML('<br/><div font-size: 20px;'\n","                    'padding-bottom:12px\"><b>Query</b>: ' + query + '</div>')\n","\n","    def show_document(self, idx, doc):\n","        \"\"\"HTML print format for document fields\"\"\"\n","        have_body_text = 'body_text' in json.loads(doc.raw)\n","        body_text = ' Full text available.' if have_body_text else ''\n","        return HTML('<div font-size: 18px; padding-bottom:10px\">' +\n","                    f'<b>Document {idx}:</b> {doc.docid} ({doc.score:1.2f}) -- ' +\n","                    f'{doc.lucene_document.get(\"authors\")} et al. ' +\n","                    f'{doc.lucene_document.get(\"title\")}. ' +\n","                    f'<a href=\"https://doi.org/{doc.lucene_document.get(\"doi\")}\">{doc.lucene_document.get(\"doi\")}</a>.'\n","                    + f'{body_text}</div>')\n","\n","    def extract_scibert(self, text, tokenizer, model):\n","        text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n","        text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n","        n_chunks = int(np.ceil(float(text_ids.size(1)) / 510))\n","        states = []\n","        for ci in range(n_chunks):\n","            text_ids_ = text_ids[0, 1 + ci * 510:1 + (ci + 1) * 510]\n","            text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n","            if text_ids[0, -1] != text_ids[0, -1]:\n","                text_ids_ = torch.cat([text_ids_, text_ids[0, -1].unsqueeze(0)])\n","            with torch.no_grad():\n","                state = model(text_ids_.unsqueeze(0))[0]\n","                state = state[:, 1:-1, :]\n","            states.append(state)\n","        state = torch.cat(states, axis=1)\n","        return text_ids, text_words, state[0]\n","\n","    def get_result_id(self, query, doc_id, searcher):\n","        \"\"\"HTML print format for the searched query\"\"\"\n","        hits = searcher.search(query)\n","        display(self.show_query(query))\n","        for i, hit in enumerate(hits):\n","            if hit.docid == doc_id:\n","                display(self.show_document(i + 1, hit))\n","                return hit\n","\n","    def cross_match(self, state1, state2):\n","        state1 = state1 / torch.sqrt((state1 ** 2).sum(1, keepdims=True))\n","        state2 = state2 / torch.sqrt((state2 ** 2).sum(1, keepdims=True))\n","        sim = (state1.unsqueeze(1) * state2.unsqueeze(0)).sum(-1)\n","        return sim\n","\n","    def show_sections(self, section, text):\n","        \"\"\"HTML print format for document subsections\"\"\"\n","        return HTML(\n","            '<div font-size: 18px; padding-bottom:10px; margin-left: 15px\">' +\n","            f'<b>{section}</b> -- {text.replace(\" ##\", \"\")} </div>')\n","\n","    def highlight_paragraph(self, ptext, rel_words, max_win=10):\n","        para = \"\"\n","        prev_idx = 0\n","        for jj in rel_words:\n","            if prev_idx > jj:\n","                continue\n","            found_start = False\n","            for kk in range(jj, prev_idx - 1, -1):\n","                if ptext[kk] == \".\" and (ptext[kk + 1][0].isupper() or ptext[kk + 1][0] == '['):\n","                    sent_start = kk\n","                    found_start = True\n","                    break\n","            if not found_start:\n","                sent_start = prev_idx - 1\n","            found_end = False\n","            for kk in range(jj, len(ptext) - 1):\n","                if ptext[kk] == \".\" and (ptext[kk + 1][0].isupper() or ptext[kk + 1][0] == '['):\n","                    sent_end = kk\n","                    found_end = True\n","                    break\n","            if not found_end:\n","                if kk >= len(ptext) - 2:\n","                    sent_end = len(ptext)\n","                else:\n","                    sent_end = jj\n","            para = para + \" \"\n","            para = para + \" \".join(ptext[prev_idx:sent_start + 1])\n","            para = para + \" <font color='blue'>\"\n","            para = para + \" \".join(ptext[sent_start + 1:sent_end])\n","            para = para + \"</font> \"\n","            prev_idx = sent_end\n","        if prev_idx < len(ptext):\n","            para = para + \" \".join(ptext[prev_idx:])\n","        return para\n","\n","    def show_results(self, question, doc_id):\n","        searcher = pysearch.SimpleSearcher(self.luceneDir)\n","        query = (question)\n","        highlighted_text = \"\"\n","        query_ids, query_words, query_state = self.extract_scibert(query, self.para_tokenizer, self.para_model)\n","        req_doc = json.loads(self.get_result_id(query, doc_id, searcher).raw)\n","        paragraph_states = []\n","        for par in tqdm(req_doc['body_text']):\n","            state = self.extract_scibert(par['text'], self.para_tokenizer, self.para_model)\n","            paragraph_states.append(state)\n","        sim_matrices = []\n","        for pid, par in tqdm(enumerate(req_doc['body_text'])):\n","            sim_score = self.cross_match(query_state, paragraph_states[pid][-1])\n","            sim_matrices.append(sim_score)\n","        paragraph_relevance = [torch.max(sim).item() for sim in sim_matrices]\n","\n","        # Select the index of top 5 paragraphs with highest relevance\n","        rel_index = np.argsort(paragraph_relevance)[-5:][::-1]\n","        for ri in np.sort(rel_index):\n","            sim = sim_matrices[ri].data.numpy()\n","\n","            # Select the two highest scoring words in the paragraph\n","            rel_words = np.sort(np.argsort(sim.max(0))[-2:][::-1])\n","            p_tokens = paragraph_states[ri][1]\n","            para = self.highlight_paragraph(p_tokens, rel_words)\n","            highlighted_text += para\n","            display(self.show_sections(req_doc[\"body_text\"][ri]['section'], para))\n","        data = {'id': doc_id, 'title': req_doc['metadata']['title'], 'text': highlighted_text}\n","        return data\n","\n","\n","class Initialize(Resource):\n","    def get(self):\n","        message = {'message': 'Hello World!'}\n","        return message, 200\n","\n","    def post(self):\n","        json_data = request.get_json()\n","        message = {'message': json_data}\n","        return message, 201\n","\n","\n","class GetAnswerBert(Resource):\n","    def post(self):\n","        json_data = request.get_json()\n","        api = BertSquad()\n","        kw_list = \"\"\n","        question = json_data['question']\n","        \n","        rich_text, warn, result = api.searchDatabase(question, kw_list, pysearch)\n","        message = {'rich_text': rich_text, 'warning': warn, 'result': result}\n","        return message, 200\n","\n","\n","class GetDetailAnswerBert(Resource):\n","    def post(self):\n","        json_data = request.get_json()\n","        api = BertSquad()\n","        question = json_data['question']\n","        doc_id = json_data['doc_id']\n","        result = api.show_results(question, doc_id)\n","        return result, 200\n","\n","\n","api.add_resource(Initialize, '/')\n","api.add_resource(GetAnswerBert, '/rich-text')\n","api.add_resource(GetDetailAnswerBert, '/detailed-text')\n","\n","if __name__ == '__main__':\n","    app.run(host=\"0.0.0.0\", port=5000, debug=False)\n","\n","# Use the below command for testing\n","# curl -H \"Content-Type: application/json\" -X POST -d '{\"question\": \"Hi\"}' http://127.0.0.1:5000/\n","from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","from flask_restful import Resource, Api\n","\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","import re\n","import gc\n","import requests\n","from bs4 import BeautifulSoup\n","import datetime\n","import dateutil.parser as dparser\n","\n","import torch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import torch\n","import transformers\n","from transformers import *\n","from transformers import BertForQuestionAnswering\n","from transformers import BertTokenizer\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","import pyserini\n","from pyserini.search import pysearch\n","from IPython.core.display import display, HTML\n","from tqdm import tqdm\n","from Bio import Entrez, Medline\n","try:\n","    from StringIO import StringIO\n","except ImportError:\n","    from io import StringIO\n","import warnings\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2/\"\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","app = Flask(__name__)\n","cors = CORS(app, resources={r\"*\": {\"origins\": \"*\"}})\n","api = Api(app)\n","\n","# Initailize tensorflow module globally if you have GPU else comment out this part. Please check the no_gpu branch. \n","def embed_useT():\n","   module = '/sentence_wise_email/module/module_useT'\n","   with tf.Graph().as_default():\n","       sentences = tf.compat.v1.placeholder(tf.string)\n","       embed = hub.Module(module)\n","       embeddings = embed(sentences)\n","       session = tf.compat.v1.train.MonitoredSession()\n","   return lambda x: session.run(embeddings, {sentences: x})\n","embed_fn = embed_useT()\n","\n","\n","class BertSquad:\n","\n","    USE_SUMMARY = True\n","    FIND_PDFS = False\n","    SEARCH_MEDRXIV = False\n","    SEARCH_PUBMED = False\n","\n","    minDate = '2020/08/13'\n","    luceneDir = '/data/indexes/lucene-index-cord19/'\n","\n","    torch_device = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","    QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","    QA_MODEL.to(torch_device)\n","    QA_MODEL.eval()\n","\n","    if USE_SUMMARY:\n","        SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","        SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","        SUMMARY_MODEL.to(torch_device)\n","        SUMMARY_MODEL.eval()\n","\n","    para_model = AutoModel.from_pretrained('monologg/biobert_v1.1_pubmed')\n","    para_tokenizer = AutoTokenizer.from_pretrained('monologg/biobert_v1.1_pubmed', do_lower_case=False)\n","    gc.collect()\n","\n","\n","    def reconstructText(self, tokens, start=0, stop=-1):\n","        tokens = tokens[start: stop]\n","        if '[SEP]' in tokens:\n","            sepind = tokens.index('[SEP]')\n","            tokens = tokens[sepind+1:]\n","        txt = ' '.join(tokens)\n","        txt = txt.replace(' ##', '')\n","        txt = txt.replace('##', '')\n","        txt = txt.strip()\n","        txt = \" \".join(txt.split())\n","        txt = txt.replace(' .', '.')\n","        txt = txt.replace('( ', '(')\n","        txt = txt.replace(' )', ')')\n","        txt = txt.replace(' - ', '-')\n","        txt_list = txt.split(' , ')\n","        txt = ''\n","        nTxtL = len(txt_list)\n","        if nTxtL == 1:\n","            return txt_list[0]\n","        newList =[]\n","        for i,t in enumerate(txt_list):\n","            if i < nTxtL -1:\n","                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n","                    newList += [t,',']\n","                else:\n","                    newList += [t, ', ']\n","            else:\n","                newList += [t]\n","        return ''.join(newList)\n","\n","\n","    def makeBERTSQuADPrediction(self, document, question):\n","        nWords = len(document.split())\n","        input_ids_all = self.QA_TOKENIZER.encode(question, document)\n","        tokens_all = self.QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n","        overlapFac = 1.1\n","        if len(input_ids_all)*overlapFac > 2048:\n","            nSearchWords = int(np.ceil(nWords/5))\n","            quarter = int(np.ceil(nWords/4))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                        ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n","                        ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n","                        ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n","                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","            \n","        elif len(input_ids_all)*overlapFac > 1536:\n","            nSearchWords = int(np.ceil(nWords/4))\n","            third = int(np.ceil(nWords/3))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                        ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n","                        ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n","                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","            \n","        elif len(input_ids_all)*overlapFac > 1024:\n","            nSearchWords = int(np.ceil(nWords/3))\n","            middle = int(np.ceil(nWords/2))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n","                        ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n","                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","        elif len(input_ids_all)*overlapFac > 512:\n","            nSearchWords = int(np.ceil(nWords/2))\n","            docSplit = document.split()\n","            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n","            input_ids = [self.QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n","        else:\n","            input_ids = [input_ids_all]\n","        absTooLong = False    \n","        \n","        answers = []\n","        cons = []\n","        for iptIds in input_ids:\n","            tokens = self.QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n","            sep_index = iptIds.index(self.QA_TOKENIZER.sep_token_id)\n","            num_seg_a = sep_index + 1\n","            num_seg_b = len(iptIds) - num_seg_a\n","            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","            assert len(segment_ids) == len(iptIds)\n","            n_ids = len(segment_ids)\n","\n","            if n_ids < 512:\n","                start_scores, end_scores = self.QA_MODEL(torch.tensor([iptIds]).to(self.torch_device),\n","                                        token_type_ids=torch.tensor([segment_ids]).to(self.torch_device))\n","            else:\n","                #this cuts off the text if its more than 512 words so it fits in model space \n","                print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n","                absTooLong = True\n","                start_scores, end_scores = self.QA_MODEL(torch.tensor([iptIds[:512]]).to(self.torch_device),\n","                                        token_type_ids=torch.tensor([segment_ids[:512]]).to(self.torch_device))\n","            start_scores = start_scores[:,1:-1]\n","            end_scores = end_scores[:,1:-1]\n","            answer_start = torch.argmax(start_scores)\n","            answer_end = torch.argmax(end_scores)\n","            answer = self.reconstructText(tokens, answer_start, answer_end+2)\n","        \n","            if answer.startswith('. ') or answer.startswith(', '):\n","                answer = answer[2:]\n","                \n","            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n","            answers.append(answer)\n","            cons.append(c)\n","        \n","        maxC = max(cons)\n","        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n","        confidence = cons[iMaxC]\n","        answer = answers[iMaxC]\n","        \n","        sep_index = tokens_all.index('[SEP]')\n","        full_txt_tokens = tokens_all[sep_index+1:]\n","        \n","        abs_returned = self.reconstructText(full_txt_tokens)\n","\n","        ans={}\n","        ans['answer'] = answer\n","        if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n","            ans['confidence'] = -1000000\n","        else:\n","            ans['confidence'] = confidence\n","        ans['abstract_bert'] = abs_returned\n","        ans['abs_too_long'] = absTooLong\n","        return ans\n","\n","\n","    def searchAbstracts(self, hit_dictionary, question):\n","        abstractResults = {}\n","        for k,v in tqdm(hit_dictionary.items()):\n","            abstract = v['abstract_full']\n","            if abstract:\n","                ans = self.makeBERTSQuADPrediction(abstract, question)\n","                if ans['answer']:\n","                    confidence = ans['confidence']\n","                    abstractResults[confidence]={}\n","                    abstractResults[confidence]['main_abstract'] = abstract\n","                    abstractResults[confidence]['answer'] = ans['answer']\n","                    abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n","                    abstractResults[confidence]['idx'] = k\n","                    abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n","                    \n","        cList = list(abstractResults.keys())\n","        if cList:\n","            maxScore = max(cList)\n","            total = 0.0\n","            exp_scores = []\n","            for c in cList:\n","                s = np.exp(c-maxScore)\n","                exp_scores.append(s)\n","            total = sum(exp_scores)\n","            for i,c in enumerate(cList):\n","                abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n","        return abstractResults\n","\n","\n","    def displayResults(self, hit_dictionary, answers, question):\n","        \n","        question_HTML = '<div font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n","        confidence = list(answers.keys())\n","        confidence.sort(reverse=True)\n","        confidence = list(answers.keys())\n","        confidence.sort(reverse=True)\n","\n","        for c in confidence:\n","            if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n","                if 'idx' not in  answers[c]:\n","                    continue\n","                rowData = []\n","                idx = answers[c]['idx']\n","                title = hit_dictionary[idx]['title']\n","                authors = hit_dictionary[idx]['authors'] + ' et al.'\n","                doi = '<a href=\"https://doi.org/'+hit_dictionary[idx]['doi']+'\" target=\"_blank\">' + title +'</a>'\n","                main_abstract = answers[c]['main_abstract']\n","                \n","                full_abs = answers[c]['abstract_bert']\n","                bert_ans = answers[c]['answer']\n","                \n","                split_abs = full_abs.split(bert_ans)\n","                sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n","                if len(split_abs) == 1:\n","                    sentance_end_pos = len(full_abs)\n","                    sentance_end =''\n","                else:\n","                    sentance_end_pos = split_abs[1].find('. ')+1\n","                    if sentance_end_pos == 0:\n","                        sentance_end = split_abs[1]\n","                    else:\n","                        sentance_end = split_abs[1][:sentance_end_pos]\n","                    \n","                answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n","                answers[c]['sentence_beginning'] = sentance_beginning\n","                answers[c]['sentence_end'] = sentance_end\n","                answers[c]['title'] = title\n","                answers[c]['doi'] = doi\n","                answers[c]['main_abstract'] = main_abstract\n","                if 'pdfLink' in hit_dictionary[idx]:\n","                    answers[c]['pdfLink'] = hit_dictionary[idx]['pdfLink']\n","\n","            else:\n","                answers.pop(c)\n","        \n","        # Please check the no_gpu branch. Comment out this part if the system doesn't support GPU\n","        ## re-rank based on semantic similarity of the answers to the question\n","        cList = list(answers.keys())\n","        allAnswers = [answers[c]['full_answer'] for c in cList]\n","\n","        messages = [question]+allAnswers\n","\n","        encoding_matrix = embed_fn(messages)\n","        gc.collect()\n","        similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n","        rankings = similarity_matrix[1:, 0]\n","\n","        for i, c in enumerate(cList):\n","            answers[rankings[i]] = answers.pop(c)\n","            \n","        # Comment till here if required\n","\n","        ## now form pandas dv\n","        confidence = list(answers.keys())\n","        confidence.sort(reverse=True)\n","        pandasData = []\n","        ranked_aswers = []\n","        for c in confidence:\n","            rowData=[]\n","            title = answers[c]['title']\n","            main_abstract = answers[c]['main_abstract']\n","            doi = answers[c]['doi']\n","            idx = answers[c]['idx']\n","            rowData += [idx]            \n","            sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='#08A293'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n","            \n","            rowData += [sentance_html, c, doi, main_abstract]\n","            pandasData.append(rowData)\n","            ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n","        \n","        if self.FIND_PDFS or self.SEARCH_MEDRXIV:\n","            pdata2 = []\n","            for rowData in pandasData:\n","                rd = rowData\n","                idx = rowData[0]\n","                if 'pdfLink' in answers[rowData[2]]:\n","                    rd += ['<a href=\"'+answers[rowData[2]]['pdfLink']+'\" target=\"_blank\">PDF Link</a>']\n","                elif self.FIND_PDFS:\n","                    if str(idx).startswith('pm_'):\n","                        pmid = idx[3:]\n","                    else:\n","                        try:\n","                            test = self.UrlReverse('https://doi.org/'+hit_dictionary[idx]['doi'])\n","                            if test is not None:\n","                                pmid = test.pmid\n","                            else:\n","                                pmid = None\n","                        except:\n","                            pmid = None\n","                    pdfLink = None\n","                    if pmid is not None:\n","                        try:\n","                            pdfLink = self.FindIt(str(pmid))\n","                        except:\n","                            pdfLink = None\n","                    if pdfLink is not None:\n","                        pdfLink = pdfLink.url\n","\n","                    if pdfLink is None:\n","\n","                        rd += ['Not Available']\n","                    else:\n","                        rd += ['<a href=\"'+pdfLink+'\" target=\"_blank\">PDF Link</a>']\n","                else:\n","                    rd += ['Not Available']\n","                pdata2.append(rowData)\n","        else:\n","            pdata2 = pandasData\n","\n","        df = pd.DataFrame(pdata2, columns=['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link', 'Abstract'])\n","        \n","        if self.USE_SUMMARY:\n","            allAnswersTxt = ' '.join(ranked_aswers[:6]).replace('\\n','')\n","            answers_input_ids = self.SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(self.torch_device)\n","            summary_ids = self.SUMMARY_MODEL.generate(answers_input_ids, num_beams=10, length_penalty=1.2, max_length=1024, min_length=64, no_repeat_ngram_size=4)\n","\n","            exec_sum = self.SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n","            execSum_HTML = '<div style=\"font-size:12px;color:#CCCC00\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n","            warning_HTML = '<div style=\"font-size:12px;padding-bottom:12px;color:#CCCC00;margin-top:1px\"> Warning: This is an autogenerated summary based on semantic search of abstracts, please examine the results before accepting this conclusion. There may be scenarios in which the summary will not be able to clearly answer the question.</div>'\n","        \n","        if self.FIND_PDFS or self.SEARCH_MEDRXIV:\n","            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link', 'Abstract'])\n","        else:\n","            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link', 'Abstract'])\n","            \n","        return exec_sum, warning_HTML, df.to_json(orient=\"records\", force_ascii=True, default_handler=None)\n","\n","\n","    def getrecord(self, id, db):\n","        handle = Entrez.efetch(db=db, id=id, rettype='Medline', retmode='text')\n","        rec = handle.read()\n","        handle.close()\n","        return rec\n","\n","\n","    def pubMedSearch(self, terms, db='pubmed', mindate='2019/12/01'):\n","        handle = Entrez.esearch(db = db, term = terms, retmax=10, mindate=mindate)\n","        record = Entrez.read(handle)\n","        record_db = {}\n","        for id in record['IdList']:\n","            try:\n","                record = self.getrecord(id,db)\n","                recfile = StringIO(record)\n","                rec = Medline.read(recfile)\n","                if 'AB' in rec and 'AU' in rec and 'LID' in rec and 'TI' in rec:\n","                    if '10.' in rec['LID'] and ' [doi]' in rec['LID']:\n","                        record_db['pm_'+id] = {}\n","                        record_db['pm_'+id]['authors'] = ' '.join(rec['AU'])\n","                        record_db['pm_'+id]['doi'] = '10.'+rec['LID'].split('10.')[1].split(' [doi]')[0]\n","                        record_db['pm_'+id]['abstract'] = rec['AB']\n","                        record_db['pm_'+id]['title'] = rec['TI']\n","            except:\n","                print(\"Problem trying to retrieve: \" + str(id))\n","            \n","        return record_db\n","    Entrez.email = 'pubmedkaggle@gmail.com'\n","\n","   \n","    def medrxivSearch(self, query, n_pages=1):\n","        results = {}\n","        q = query\n","        for x in range(n_pages):\n","            PARAMS = {\n","                'page': x\n","            }\n","            r = requests.get('https://www.medrxiv.org/search/' + q, params = PARAMS)\n","            content = r.text\n","            page = BeautifulSoup(content, 'lxml')\n","            \n","            for entry in page.find_all(\"a\", attrs={\"class\": \"highwire-cite-linked-title\"}):\n","                title = \"\"\n","                url = \"\"\n","                pubDate = \"\"\n","                journal = None\n","                abstract = \"\"\n","                authors = []\n","                database = \"medRxiv\"\n","                \n","                url = \"https://www.medrxiv.org\" + entry.get('href')\n","                \n","                request_entry = requests.get(url)\n","                content_entry = request_entry.text\n","                page_entry = BeautifulSoup(content_entry, 'lxml')\n","                doi = page_entry.find(\"span\", attrs={\"class\": \"highwire-cite-metadata-doi\"}).text.split('doi.org/')[-1]\n","\n","                #getting pubDate\n","                pubDate = page_entry.find_all(\"div\", attrs = {\"class\": \"pane-content\"})\n","                pubDate = pubDate[10]\n","                pubDate = str(dparser.parse(pubDate, fuzzy = True))\n","                pubDate = datetime.datetime.strptime(pubDate, '%Y-%m-%d %H:%M:%S')\n","                pubDate = pubDate.strftime('%b %d %Y')\n","                date = pubDate.split()\n","                month = date[0]\n","                day = date[1]\n","                year = date[2]\n","                pubDate = {\n","                    'year': year,\n","                    'month': month,\n","                    'day': day\n","                }\n","\n","                #getting title\n","                title = page_entry.find(\"h1\", attrs={\"class\": \"highwire-cite-title\"}).text\n","                #getting abstract\n","                abstract = page_entry.find(\"p\", attrs = {\"id\": \"p-2\"}).text.replace('\\n', ' ')\n","                #getting authors \n","                givenNames = page_entry.find_all(\"span\", attrs={\"class\": \"nlm-given-names\"})\n","                surnames = page_entry.find_all(\"span\",  attrs={\"class\": \"nlm-surname\"})\n","                names = list(zip(givenNames,surnames))\n","                for author in names:\n","                    name = author[0].text + ' ' + author[1].text\n","                    if name not in authors:\n","                        authors.append(name)\n","                \n","                result = {\n","                    'title': title,\n","                    'url': url,\n","                    'pubDate': pubDate,\n","                    'journal': journal,\n","                    'abstract': abstract,\n","                    'authors': authors[0],\n","                    'database': database,\n","                    'doi': doi,\n","                    'pdfLink': url+'.full.pdf'\n","                }\n","                results['mrx_'+result['doi'].split('/')[-1]] = result\n","                #break\n","\n","        return results\n","\n","\n","    def searchDatabase(self, question, keywords, pysearch):\n","        ## search the lucene database with a combination of the question and the keywords\n","        pm_kw = ''\n","        minDate='2019/12/01'\n","        k=20\n","        \n","        searcher = pysearch.SimpleSearcher(self.luceneDir)\n","        hits = searcher.search(question + '. ' + keywords, k=k)\n","        n_hits = len(hits)\n","        ## collect the relevant data in a hit dictionary\n","        hit_dictionary = {}\n","        for i in range(0, n_hits):\n","            doc_json = json.loads(hits[i].raw)\n","            idx = str(hits[i].docid)\n","            hit_dictionary[idx] = doc_json\n","            hit_dictionary[idx]['title'] = hits[i].lucene_document.get(\"title\")\n","            hit_dictionary[idx]['authors'] = hits[i].lucene_document.get(\"authors\")\n","            hit_dictionary[idx]['doi'] = hits[i].lucene_document.get(\"doi\")\n","            \n","        titleList = [h['title'] for h in hit_dictionary.values()]\n","        \n","        # search for PubMed and medArxiv data dynamically\n","        if pm_kw:\n","            if SEARCH_PUBMED:\n","                new_hits = pubMedSearch(pm_kw, db='pubmed', mindate=minDate)\n","                for id,h in new_hits.items():\n","                    if h['title'] not in titleList:\n","                        titleList.append(h['title'])\n","                    hit_dictionary[id] = h\n","            if SEARCH_MEDRXIV:\n","                new_hits = medrxivSearch(pm_kw)\n","                for id,h in new_hits.items():\n","                    if h['title'] not in titleList:\n","                        titleList.append(h['title'])\n","                    hit_dictionary[id] = h\n","        \n","        ## scrub the abstracts in prep for BERT-SQuAD\n","        for idx,v in hit_dictionary.items():\n","\n","            try:\n","                abs_dirty = v['abstract']\n","            except KeyError:\n","                print(\"Sorry! No abstract found.\")\n","                abs_dirty = ''\n","                # uncomment the code if required search on body_text also. Will impact processing time\n","\n","    #             if v['has_full_text'] == True:\n","    #                 print(v['paper_id'])\n","    #                 abs_dirty = v['body_text']\n","    #             else:\n","    #                 print(v.keys())\n","    #         abs_dirty = ''\n","    #         abs_dirty = v['abstract']\n","\n","            # looks like the abstract value can be an empty list\n","            v['abstract_paragraphs'] = []\n","            v['abstract_full'] = ''\n","\n","            if abs_dirty:\n","                # if it is a list, then the only entry is a dictionary where text is in 'text' key it is broken up by paragraph if it is in that form.  \n","                # make lists for every paragraph that is full abstract text as both could be valuable for BERT derrived QA\n","\n","                if isinstance(abs_dirty, list):\n","                    for p in abs_dirty:\n","                        v['abstract_paragraphs'].append(p['text'])\n","                        v['abstract_full'] += p['text'] + ' \\n\\n'\n","\n","                # in some cases the abstract can be straight up text so we can actually leave that alone\n","                if isinstance(abs_dirty, str):\n","                    v['abstract_paragraphs'].append(abs_dirty)\n","                    v['abstract_full'] += abs_dirty + ' \\n\\n'\n","        \n","        ## Search collected abstracts with BERT-SQuAD\n","        answers = self.searchAbstracts(hit_dictionary, question)\n","        ## displaying results in a nice format\n","        return self.displayResults(hit_dictionary, answers, question)\n","\n","\n","    def show_query(self, query):\n","        \"\"\"HTML print format for the searched query\"\"\"\n","        return HTML('<br/><div font-size: 20px;'\n","                    'padding-bottom:12px\"><b>Query</b>: ' + query + '</div>')\n","\n","    def show_document(self, idx, doc):\n","        \"\"\"HTML print format for document fields\"\"\"\n","        have_body_text = 'body_text' in json.loads(doc.raw)\n","        body_text = ' Full text available.' if have_body_text else ''\n","        return HTML('<div font-size: 18px; padding-bottom:10px\">' +\n","                    f'<b>Document {idx}:</b> {doc.docid} ({doc.score:1.2f}) -- ' +\n","                    f'{doc.lucene_document.get(\"authors\")} et al. ' +\n","                    f'{doc.lucene_document.get(\"title\")}. ' +\n","                    f'<a href=\"https://doi.org/{doc.lucene_document.get(\"doi\")}\">{doc.lucene_document.get(\"doi\")}</a>.'\n","                    + f'{body_text}</div>')\n","\n","    def extract_scibert(self, text, tokenizer, model):\n","        text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n","        text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n","        n_chunks = int(np.ceil(float(text_ids.size(1)) / 510))\n","        states = []\n","        for ci in range(n_chunks):\n","            text_ids_ = text_ids[0, 1 + ci * 510:1 + (ci + 1) * 510]\n","            text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n","            if text_ids[0, -1] != text_ids[0, -1]:\n","                text_ids_ = torch.cat([text_ids_, text_ids[0, -1].unsqueeze(0)])\n","            with torch.no_grad():\n","                state = model(text_ids_.unsqueeze(0))[0]\n","                state = state[:, 1:-1, :]\n","            states.append(state)\n","        state = torch.cat(states, axis=1)\n","        return text_ids, text_words, state[0]\n","\n","    def get_result_id(self, query, doc_id, searcher):\n","        \"\"\"HTML print format for the searched query\"\"\"\n","        hits = searcher.search(query)\n","        display(self.show_query(query))\n","        for i, hit in enumerate(hits):\n","            if hit.docid == doc_id:\n","                display(self.show_document(i + 1, hit))\n","                return hit\n","\n","    def cross_match(self, state1, state2):\n","        state1 = state1 / torch.sqrt((state1 ** 2).sum(1, keepdims=True))\n","        state2 = state2 / torch.sqrt((state2 ** 2).sum(1, keepdims=True))\n","        sim = (state1.unsqueeze(1) * state2.unsqueeze(0)).sum(-1)\n","        return sim\n","\n","    def show_sections(self, section, text):\n","        \"\"\"HTML print format for document subsections\"\"\"\n","        return HTML(\n","            '<div font-size: 18px; padding-bottom:10px; margin-left: 15px\">' +\n","            f'<b>{section}</b> -- {text.replace(\" ##\", \"\")} </div>')\n","\n","    def highlight_paragraph(self, ptext, rel_words, max_win=10):\n","        para = \"\"\n","        prev_idx = 0\n","        for jj in rel_words:\n","            if prev_idx > jj:\n","                continue\n","            found_start = False\n","            for kk in range(jj, prev_idx - 1, -1):\n","                if ptext[kk] == \".\" and (ptext[kk + 1][0].isupper() or ptext[kk + 1][0] == '['):\n","                    sent_start = kk\n","                    found_start = True\n","                    break\n","            if not found_start:\n","                sent_start = prev_idx - 1\n","            found_end = False\n","            for kk in range(jj, len(ptext) - 1):\n","                if ptext[kk] == \".\" and (ptext[kk + 1][0].isupper() or ptext[kk + 1][0] == '['):\n","                    sent_end = kk\n","                    found_end = True\n","                    break\n","            if not found_end:\n","                if kk >= len(ptext) - 2:\n","                    sent_end = len(ptext)\n","                else:\n","                    sent_end = jj\n","            para = para + \" \"\n","            para = para + \" \".join(ptext[prev_idx:sent_start + 1])\n","            para = para + \" <font color='blue'>\"\n","            para = para + \" \".join(ptext[sent_start + 1:sent_end])\n","            para = para + \"</font> \"\n","            prev_idx = sent_end\n","        if prev_idx < len(ptext):\n","            para = para + \" \".join(ptext[prev_idx:])\n","        return para\n","\n","    def show_results(self, question, doc_id):\n","        searcher = pysearch.SimpleSearcher(self.luceneDir)\n","        query = (question)\n","        highlighted_text = \"\"\n","        query_ids, query_words, query_state = self.extract_scibert(query, self.para_tokenizer, self.para_model)\n","        req_doc = json.loads(self.get_result_id(query, doc_id, searcher).raw)\n","        paragraph_states = []\n","        for par in tqdm(req_doc['body_text']):\n","            state = self.extract_scibert(par['text'], self.para_tokenizer, self.para_model)\n","            paragraph_states.append(state)\n","        sim_matrices = []\n","        for pid, par in tqdm(enumerate(req_doc['body_text'])):\n","            sim_score = self.cross_match(query_state, paragraph_states[pid][-1])\n","            sim_matrices.append(sim_score)\n","        paragraph_relevance = [torch.max(sim).item() for sim in sim_matrices]\n","\n","        # Select the index of top 5 paragraphs with highest relevance\n","        rel_index = np.argsort(paragraph_relevance)[-5:][::-1]\n","        for ri in np.sort(rel_index):\n","            sim = sim_matrices[ri].data.numpy()\n","\n","            # Select the two highest scoring words in the paragraph\n","            rel_words = np.sort(np.argsort(sim.max(0))[-2:][::-1])\n","            p_tokens = paragraph_states[ri][1]\n","            para = self.highlight_paragraph(p_tokens, rel_words)\n","            highlighted_text += para\n","            display(self.show_sections(req_doc[\"body_text\"][ri]['section'], para))\n","        data = {'id': doc_id, 'title': req_doc['metadata']['title'], 'text': highlighted_text}\n","        return data\n","\n","\n","class Initialize(Resource):\n","    def get(self):\n","        message = {'message': 'Hello World!'}\n","        return message, 200\n","\n","    def post(self):\n","        json_data = request.get_json()\n","        message = {'message': json_data}\n","        return message, 201\n","\n","\n","class GetAnswerBert(Resource):\n","    def post(self):\n","        json_data = request.get_json()\n","        api = BertSquad()\n","        kw_list = \"\"\n","        question = json_data['question']\n","        \n","        rich_text, warn, result = api.searchDatabase(question, kw_list, pysearch)\n","        message = {'rich_text': rich_text, 'warning': warn, 'result': result}\n","        return message, 200\n","\n","\n","class GetDetailAnswerBert(Resource):\n","    def post(self):\n","        json_data = request.get_json()\n","        api = BertSquad()\n","        question = json_data['question']\n","        doc_id = json_data['doc_id']\n","        result = api.show_results(question, doc_id)\n","        return result, 200\n","\n","\n","api.add_resource(Initialize, '/')\n","api.add_resource(GetAnswerBert, '/rich-text')\n","api.add_resource(GetDetailAnswerBert, '/detailed-text')\n","\n","if __name__ == '__main__':\n","    app.run(host=\"0.0.0.0\", port=5000, debug=False)\n","\n","# Use the below command for testing\n","# curl -H \"Content-Type: application/json\" -X POST -d '{\"question\": \"Hi\"}' http://127.0.0.1:5000/\n"],"execution_count":null,"outputs":[]}]}